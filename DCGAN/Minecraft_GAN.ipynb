{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minecraft-GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sesi2pPKAaqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "fc2e1d44-1392-446d-a1f6-3cfded27d1fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! rm -r temp\n",
        "! mkdir temp\n",
        "#! rm -r \"/content/drive/My Drive/Minecraft/images\"\n",
        "#! rm -r \"/content/drive/My Drive/Minecraft/checkpoint\"\n",
        "#! mkdir \"/content/drive/My Drive/Minecraft/images\"\n",
        "#! mkdir \"/content/drive/My Drive/Minecraft/checkpoint\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "rm: cannot remove 'temp': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0isrhYXZMd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "996f65ea-6342-4203-94a8-df7c123b6c51"
      },
      "source": [
        "! pip install tensorflow-gpu==1.15\n",
        "! pip install keras==2.1.3\n",
        "! mkdir \"/content/drive/My Drive/Minecraft/1\"\n",
        "! mkdir \"/content/drive/My Drive/Minecraft/1/images\"\n",
        "! mkdir \"/content/drive/My Drive/Minecraft/1/checkpoint\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.9.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.29.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.15) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=5ffba3976bfc1603fcf1fde6e5518c992e04b9d3486aadbb5d6539bceb7d839d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement tensorflow-estimator<2.3.0,>=2.2.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting keras==2.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.3) (1.12.0)\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.3.1\n",
            "    Uninstalling Keras-2.3.1:\n",
            "      Successfully uninstalled Keras-2.3.1\n",
            "Successfully installed keras-2.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad30sFfKY7fH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "8d2dbde5-a1bc-4cbb-82c7-e316a38529b8"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, ZeroPadding3D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D, UpSampling3D, Conv3D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam, SGD\n",
        "import os\n",
        "from tensorboardcolab import *\n",
        "tbc=TensorBoardColab()\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0' \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())\n",
        "import sys\n",
        "import datetime\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "with np.load(\"drive/My Drive/Minecraft/largeNorm.npz\") as data:\n",
        "    images = data['a']\n",
        "print(tf.__version__)\n",
        "\n",
        "shuffle(images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://9b29f5f6ac0b.ngrok.io\n",
            "/device:GPU:0\n",
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4cYwCuCZGds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DCGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 256\n",
        "        self.img_cols = 16\n",
        "        self.img_depth = 16\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.img_depth, self.channels)\n",
        "        self.latent_dim = 256\n",
        "\n",
        "        optimizerD = Adam(0.0005)\n",
        "        optimizerG = Adam(0.0001, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizerD,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        valid = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizerG)\n",
        "    \n",
        "    def write_log(self, name, value, epoch):\n",
        "        tbc.save_value(\"Loss\", name, epoch, value)\n",
        "        tbc.flush_line(name)\n",
        "        tbc.close()\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 4 * 4 * 64, input_dim=self.latent_dim, activation=\"relu\"))\n",
        "        model.add(Reshape((64, 4, 4, 128)))\n",
        "        model.add(UpSampling3D())\n",
        "        model.add(Conv3D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling3D())\n",
        "        model.add(Conv3D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv3D(1, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv3D(256, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        \n",
        "        model.add(Conv3D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding3D(padding=((0,1),(0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        \n",
        "        model.add(Conv3D(64, kernel_size=4, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding3D(padding=((0,1),(0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        \n",
        "        model.add(Conv3D(32, kernel_size=4, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv3D(16, kernel_size=4, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        \n",
        "        model.add(Conv3D(8, kernel_size=4, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        \n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=16, save_interval=500):\n",
        "\n",
        "        # Load the dataset\n",
        "        X_train = images\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        #print(X_train[0][0])\n",
        "        #X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=4)\n",
        "        #print(X_train[0][0])\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.full((batch_size, 1), 0.9)\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            #epoch += 24000\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise and generate a batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator (real classified as ones and generated as zeros)\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            #print(type(d_loss_real))\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator (wants discriminator to mistake images as real)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "            #print (\"%d [D Loss Real: %f, D Loss Fake: %f\" % (epoch, d_loss_real, d_loss_fake))\n",
        "            #print(d_loss_real)\n",
        "            #print(d_loss_fake)\n",
        "            self.write_log(\"g_loss\", g_loss, epoch)\n",
        "            self.write_log(\"d_loss\", d_loss[0], epoch)\n",
        "            self.write_log(\"Accuracy\", d_loss[1]*100, epoch)\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "            if epoch % (2*save_interval) == 0:\n",
        "                self.combined.save_weights('drive/My Drive/Minecraft/1/checkpoint/mnist-%d.h5' % epoch)\n",
        "                #self.combined.save_weights('temp/mnist-%d.h5' % epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        #gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        np.save(\"drive/My Drive/Minecraft/1/images/mnist_%d.npy\" % epoch, gen_imgs)\n",
        "        #np.save(\"temp/mnist_%d.npy\" % epoch, gen_imgs)\n",
        "        '''\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"C:/Users/adamj/Documents/MinecraftTerrainGeneration/DCGAN/images/mnist_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "        '''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNecgz6WZHxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86887a0f-f38a-4e60-eb0b-d75177a27647"
      },
      "source": [
        "dcgan = DCGAN()\n",
        "#dcgan.combined.load_weights(\"/content/drive/My Drive/Minecraft/checkpoint/mnist-24000.h5\")\n",
        "dcgan.train(epochs=100000, batch_size=16, save_interval=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_10 (Conv3D)           (None, 128, 8, 8, 256)    16640     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 128, 8, 8, 256)    0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 128, 8, 8, 256)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_11 (Conv3D)           (None, 64, 4, 4, 128)     2097280   \n",
            "_________________________________________________________________\n",
            "zero_padding3d_3 (ZeroPaddin (None, 65, 5, 5, 128)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 65, 5, 5, 128)     512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 65, 5, 5, 128)     0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 65, 5, 5, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_12 (Conv3D)           (None, 33, 3, 3, 64)      524352    \n",
            "_________________________________________________________________\n",
            "zero_padding3d_4 (ZeroPaddin (None, 34, 4, 4, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 34, 4, 4, 64)      256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 34, 4, 4, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 34, 4, 4, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_13 (Conv3D)           (None, 17, 2, 2, 32)      131104    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 17, 2, 2, 32)      128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)   (None, 17, 2, 2, 32)      0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 17, 2, 2, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_14 (Conv3D)           (None, 17, 2, 2, 16)      32784     \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 17, 2, 2, 16)      64        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)   (None, 17, 2, 2, 16)      0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 17, 2, 2, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv3d_15 (Conv3D)           (None, 17, 2, 2, 8)       8200      \n",
            "_________________________________________________________________\n",
            "batch_normalization_12 (Batc (None, 17, 2, 2, 8)       32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)   (None, 17, 2, 2, 8)       0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 17, 2, 2, 8)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 544)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 545       \n",
            "=================================================================\n",
            "Total params: 2,811,897\n",
            "Trainable params: 2,811,401\n",
            "Non-trainable params: 496\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 131072)            33685504  \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 64, 4, 4, 128)     0         \n",
            "_________________________________________________________________\n",
            "up_sampling3d_3 (UpSampling3 (None, 128, 8, 8, 128)    0         \n",
            "_________________________________________________________________\n",
            "conv3d_16 (Conv3D)           (None, 128, 8, 8, 128)    1048704   \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 128, 8, 8, 128)    512       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128, 8, 8, 128)    0         \n",
            "_________________________________________________________________\n",
            "up_sampling3d_4 (UpSampling3 (None, 256, 16, 16, 128)  0         \n",
            "_________________________________________________________________\n",
            "conv3d_17 (Conv3D)           (None, 256, 16, 16, 64)   524352    \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 256, 16, 16, 64)   256       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256, 16, 16, 64)   0         \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 256, 16, 16, 1)    4097      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256, 16, 16, 1)    0         \n",
            "=================================================================\n",
            "Total params: 35,263,425\n",
            "Trainable params: 35,263,041\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.809229, acc.: 18.75%] [G loss: 0.864955]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:953: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 [D loss: 1.348994, acc.: 0.00%] [G loss: 0.476868]\n",
            "2 [D loss: 0.783331, acc.: 6.25%] [G loss: 0.670638]\n",
            "3 [D loss: 0.833014, acc.: 31.25%] [G loss: 0.845718]\n",
            "4 [D loss: 0.714874, acc.: 34.38%] [G loss: 0.969436]\n",
            "5 [D loss: 0.701949, acc.: 34.38%] [G loss: 1.088436]\n",
            "6 [D loss: 0.665705, acc.: 34.38%] [G loss: 1.314684]\n",
            "7 [D loss: 0.634192, acc.: 46.88%] [G loss: 1.137107]\n",
            "8 [D loss: 0.667128, acc.: 37.50%] [G loss: 1.525097]\n",
            "9 [D loss: 0.617384, acc.: 43.75%] [G loss: 1.679902]\n",
            "10 [D loss: 0.450204, acc.: 46.88%] [G loss: 1.989478]\n",
            "11 [D loss: 0.380173, acc.: 46.88%] [G loss: 1.902729]\n",
            "12 [D loss: 0.468255, acc.: 37.50%] [G loss: 0.994180]\n",
            "13 [D loss: 0.429614, acc.: 43.75%] [G loss: 0.393427]\n",
            "14 [D loss: 0.364573, acc.: 46.88%] [G loss: 0.475755]\n",
            "15 [D loss: 0.273194, acc.: 50.00%] [G loss: 0.382435]\n",
            "16 [D loss: 0.270241, acc.: 50.00%] [G loss: 0.435992]\n",
            "17 [D loss: 1.415858, acc.: 0.00%] [G loss: 0.405355]\n",
            "18 [D loss: 1.100399, acc.: 0.00%] [G loss: 1.372739]\n",
            "19 [D loss: 0.785695, acc.: 43.75%] [G loss: 1.840556]\n",
            "20 [D loss: 1.147120, acc.: 40.62%] [G loss: 1.438365]\n",
            "21 [D loss: 1.063417, acc.: 40.62%] [G loss: 1.108741]\n",
            "22 [D loss: 0.785690, acc.: 37.50%] [G loss: 0.821545]\n",
            "23 [D loss: 0.775960, acc.: 25.00%] [G loss: 0.779669]\n",
            "24 [D loss: 0.751930, acc.: 28.12%] [G loss: 0.833908]\n",
            "25 [D loss: 0.770855, acc.: 18.75%] [G loss: 0.991416]\n",
            "26 [D loss: 0.811021, acc.: 21.88%] [G loss: 0.788067]\n",
            "27 [D loss: 0.765598, acc.: 21.88%] [G loss: 1.043638]\n",
            "28 [D loss: 0.690683, acc.: 28.12%] [G loss: 0.864762]\n",
            "29 [D loss: 0.701512, acc.: 28.12%] [G loss: 0.956018]\n",
            "30 [D loss: 0.831465, acc.: 37.50%] [G loss: 0.916774]\n",
            "31 [D loss: 0.654971, acc.: 43.75%] [G loss: 0.897188]\n",
            "32 [D loss: 0.772454, acc.: 37.50%] [G loss: 0.802295]\n",
            "33 [D loss: 0.692787, acc.: 34.38%] [G loss: 0.788418]\n",
            "34 [D loss: 0.805812, acc.: 31.25%] [G loss: 0.820224]\n",
            "35 [D loss: 0.766171, acc.: 28.12%] [G loss: 0.997091]\n",
            "36 [D loss: 0.717589, acc.: 25.00%] [G loss: 0.790139]\n",
            "37 [D loss: 0.636283, acc.: 40.62%] [G loss: 0.938260]\n",
            "38 [D loss: 0.781728, acc.: 28.12%] [G loss: 0.904423]\n",
            "39 [D loss: 0.907856, acc.: 25.00%] [G loss: 0.862033]\n",
            "40 [D loss: 0.770464, acc.: 21.88%] [G loss: 0.954231]\n",
            "41 [D loss: 0.747216, acc.: 28.12%] [G loss: 0.792894]\n",
            "42 [D loss: 0.774708, acc.: 28.12%] [G loss: 0.833586]\n",
            "43 [D loss: 0.776177, acc.: 25.00%] [G loss: 0.926898]\n",
            "44 [D loss: 0.672516, acc.: 43.75%] [G loss: 0.787294]\n",
            "45 [D loss: 0.686514, acc.: 43.75%] [G loss: 0.848699]\n",
            "46 [D loss: 0.743970, acc.: 34.38%] [G loss: 0.853008]\n",
            "47 [D loss: 0.691219, acc.: 43.75%] [G loss: 0.842030]\n",
            "48 [D loss: 0.735241, acc.: 37.50%] [G loss: 0.877327]\n",
            "49 [D loss: 0.708766, acc.: 37.50%] [G loss: 0.914562]\n",
            "50 [D loss: 0.820150, acc.: 25.00%] [G loss: 0.878292]\n",
            "51 [D loss: 0.887548, acc.: 25.00%] [G loss: 0.906403]\n",
            "52 [D loss: 0.756580, acc.: 28.12%] [G loss: 0.854313]\n",
            "53 [D loss: 0.594572, acc.: 43.75%] [G loss: 0.930212]\n",
            "54 [D loss: 0.768031, acc.: 31.25%] [G loss: 0.918443]\n",
            "55 [D loss: 0.733412, acc.: 28.12%] [G loss: 0.915734]\n",
            "56 [D loss: 0.582235, acc.: 43.75%] [G loss: 0.919883]\n",
            "57 [D loss: 0.784734, acc.: 28.12%] [G loss: 0.871164]\n",
            "58 [D loss: 0.705606, acc.: 34.38%] [G loss: 0.796010]\n",
            "59 [D loss: 0.727143, acc.: 34.38%] [G loss: 0.697795]\n",
            "60 [D loss: 0.760008, acc.: 31.25%] [G loss: 1.039357]\n",
            "61 [D loss: 0.654473, acc.: 43.75%] [G loss: 0.710607]\n",
            "62 [D loss: 0.710108, acc.: 34.38%] [G loss: 0.776884]\n",
            "63 [D loss: 0.749773, acc.: 31.25%] [G loss: 0.840719]\n",
            "64 [D loss: 0.790237, acc.: 34.38%] [G loss: 0.845473]\n",
            "65 [D loss: 0.775154, acc.: 37.50%] [G loss: 0.970857]\n",
            "66 [D loss: 0.646177, acc.: 28.12%] [G loss: 0.828063]\n",
            "67 [D loss: 0.851407, acc.: 25.00%] [G loss: 0.867398]\n",
            "68 [D loss: 0.757797, acc.: 31.25%] [G loss: 0.751032]\n",
            "69 [D loss: 0.658935, acc.: 37.50%] [G loss: 1.005926]\n",
            "70 [D loss: 0.840613, acc.: 21.88%] [G loss: 0.872637]\n",
            "71 [D loss: 0.687535, acc.: 34.38%] [G loss: 0.941780]\n",
            "72 [D loss: 0.683933, acc.: 34.38%] [G loss: 0.753271]\n",
            "73 [D loss: 0.610968, acc.: 43.75%] [G loss: 0.700024]\n",
            "74 [D loss: 0.768244, acc.: 28.12%] [G loss: 0.811458]\n",
            "75 [D loss: 0.739366, acc.: 34.38%] [G loss: 0.804966]\n",
            "76 [D loss: 0.719126, acc.: 34.38%] [G loss: 0.848355]\n",
            "77 [D loss: 0.727284, acc.: 34.38%] [G loss: 0.825388]\n",
            "78 [D loss: 0.838287, acc.: 28.12%] [G loss: 0.734725]\n",
            "79 [D loss: 0.723928, acc.: 25.00%] [G loss: 1.030297]\n",
            "80 [D loss: 0.797766, acc.: 18.75%] [G loss: 0.678809]\n",
            "81 [D loss: 0.798153, acc.: 25.00%] [G loss: 0.790157]\n",
            "82 [D loss: 0.781557, acc.: 21.88%] [G loss: 0.766859]\n",
            "83 [D loss: 0.701785, acc.: 34.38%] [G loss: 0.834943]\n",
            "84 [D loss: 0.778494, acc.: 25.00%] [G loss: 0.862991]\n",
            "85 [D loss: 0.673298, acc.: 37.50%] [G loss: 0.903716]\n",
            "86 [D loss: 0.739292, acc.: 25.00%] [G loss: 0.879571]\n",
            "87 [D loss: 0.706560, acc.: 40.62%] [G loss: 1.071457]\n",
            "88 [D loss: 0.823492, acc.: 21.88%] [G loss: 0.828726]\n",
            "89 [D loss: 0.749652, acc.: 31.25%] [G loss: 0.858546]\n",
            "90 [D loss: 0.748816, acc.: 34.38%] [G loss: 0.851630]\n",
            "91 [D loss: 0.748697, acc.: 31.25%] [G loss: 0.771565]\n",
            "92 [D loss: 0.713888, acc.: 31.25%] [G loss: 0.844981]\n",
            "93 [D loss: 0.777319, acc.: 37.50%] [G loss: 0.771327]\n",
            "94 [D loss: 0.719067, acc.: 37.50%] [G loss: 0.728097]\n",
            "95 [D loss: 0.804389, acc.: 21.88%] [G loss: 0.886898]\n",
            "96 [D loss: 0.738462, acc.: 31.25%] [G loss: 0.963503]\n",
            "97 [D loss: 0.676384, acc.: 40.62%] [G loss: 0.728002]\n",
            "98 [D loss: 0.724992, acc.: 28.12%] [G loss: 0.756704]\n",
            "99 [D loss: 0.747359, acc.: 37.50%] [G loss: 0.802900]\n",
            "100 [D loss: 0.693484, acc.: 34.38%] [G loss: 0.817953]\n",
            "101 [D loss: 0.724239, acc.: 18.75%] [G loss: 0.840517]\n",
            "102 [D loss: 0.660779, acc.: 40.62%] [G loss: 0.799440]\n",
            "103 [D loss: 0.726787, acc.: 34.38%] [G loss: 0.859737]\n",
            "104 [D loss: 0.706753, acc.: 31.25%] [G loss: 0.790643]\n",
            "105 [D loss: 0.656416, acc.: 40.62%] [G loss: 0.956016]\n",
            "106 [D loss: 0.737952, acc.: 28.12%] [G loss: 0.760975]\n",
            "107 [D loss: 0.678364, acc.: 40.62%] [G loss: 0.935231]\n",
            "108 [D loss: 0.695934, acc.: 43.75%] [G loss: 0.792870]\n",
            "109 [D loss: 0.754895, acc.: 31.25%] [G loss: 0.805131]\n",
            "110 [D loss: 0.748828, acc.: 25.00%] [G loss: 0.779334]\n",
            "111 [D loss: 0.763472, acc.: 21.88%] [G loss: 0.887374]\n",
            "112 [D loss: 0.785382, acc.: 28.12%] [G loss: 0.839116]\n",
            "113 [D loss: 0.781405, acc.: 31.25%] [G loss: 0.840569]\n",
            "114 [D loss: 0.644135, acc.: 43.75%] [G loss: 0.825358]\n",
            "115 [D loss: 0.724294, acc.: 37.50%] [G loss: 0.882829]\n",
            "116 [D loss: 0.707510, acc.: 37.50%] [G loss: 0.875908]\n",
            "117 [D loss: 0.678960, acc.: 34.38%] [G loss: 0.894340]\n",
            "118 [D loss: 0.747653, acc.: 25.00%] [G loss: 0.909258]\n",
            "119 [D loss: 0.708111, acc.: 28.12%] [G loss: 0.838800]\n",
            "120 [D loss: 0.749650, acc.: 31.25%] [G loss: 0.801269]\n",
            "121 [D loss: 0.694076, acc.: 28.12%] [G loss: 0.754924]\n",
            "122 [D loss: 0.800873, acc.: 28.12%] [G loss: 0.747363]\n",
            "123 [D loss: 0.799389, acc.: 31.25%] [G loss: 0.760143]\n",
            "124 [D loss: 0.716845, acc.: 37.50%] [G loss: 0.778615]\n",
            "125 [D loss: 0.696421, acc.: 31.25%] [G loss: 0.724920]\n",
            "126 [D loss: 0.769874, acc.: 31.25%] [G loss: 0.813185]\n",
            "127 [D loss: 0.757529, acc.: 37.50%] [G loss: 0.818178]\n",
            "128 [D loss: 0.704150, acc.: 34.38%] [G loss: 0.648345]\n",
            "129 [D loss: 0.667273, acc.: 31.25%] [G loss: 0.885420]\n",
            "130 [D loss: 0.704983, acc.: 34.38%] [G loss: 0.749230]\n",
            "131 [D loss: 0.737580, acc.: 31.25%] [G loss: 0.707141]\n",
            "132 [D loss: 0.716683, acc.: 37.50%] [G loss: 0.743922]\n",
            "133 [D loss: 0.700512, acc.: 34.38%] [G loss: 0.896303]\n",
            "134 [D loss: 0.739031, acc.: 28.12%] [G loss: 0.691832]\n",
            "135 [D loss: 0.749758, acc.: 28.12%] [G loss: 0.799270]\n",
            "136 [D loss: 0.678762, acc.: 37.50%] [G loss: 0.854527]\n",
            "137 [D loss: 0.694123, acc.: 31.25%] [G loss: 0.889067]\n",
            "138 [D loss: 0.739434, acc.: 28.12%] [G loss: 0.947906]\n",
            "139 [D loss: 0.766710, acc.: 31.25%] [G loss: 0.843487]\n",
            "140 [D loss: 0.731080, acc.: 25.00%] [G loss: 0.720494]\n",
            "141 [D loss: 0.753522, acc.: 28.12%] [G loss: 0.864395]\n",
            "142 [D loss: 0.736901, acc.: 31.25%] [G loss: 0.883348]\n",
            "143 [D loss: 0.688730, acc.: 37.50%] [G loss: 0.808330]\n",
            "144 [D loss: 0.726343, acc.: 31.25%] [G loss: 0.883057]\n",
            "145 [D loss: 0.752188, acc.: 25.00%] [G loss: 0.881632]\n",
            "146 [D loss: 0.770513, acc.: 28.12%] [G loss: 0.906862]\n",
            "147 [D loss: 0.682917, acc.: 37.50%] [G loss: 0.776591]\n",
            "148 [D loss: 0.695298, acc.: 40.62%] [G loss: 0.899778]\n",
            "149 [D loss: 0.703242, acc.: 40.62%] [G loss: 0.936091]\n",
            "150 [D loss: 0.679743, acc.: 46.88%] [G loss: 0.811296]\n",
            "151 [D loss: 0.708544, acc.: 37.50%] [G loss: 0.649544]\n",
            "152 [D loss: 0.697305, acc.: 40.62%] [G loss: 0.835585]\n",
            "153 [D loss: 0.739899, acc.: 34.38%] [G loss: 0.772210]\n",
            "154 [D loss: 0.729916, acc.: 34.38%] [G loss: 0.776003]\n",
            "155 [D loss: 0.676546, acc.: 43.75%] [G loss: 0.827100]\n",
            "156 [D loss: 0.669787, acc.: 37.50%] [G loss: 0.751875]\n",
            "157 [D loss: 0.749858, acc.: 25.00%] [G loss: 0.798617]\n",
            "158 [D loss: 0.714181, acc.: 28.12%] [G loss: 0.871642]\n",
            "159 [D loss: 0.703679, acc.: 34.38%] [G loss: 0.817161]\n",
            "160 [D loss: 0.760254, acc.: 34.38%] [G loss: 0.697363]\n",
            "161 [D loss: 0.731051, acc.: 25.00%] [G loss: 0.849682]\n",
            "162 [D loss: 0.731233, acc.: 37.50%] [G loss: 0.927002]\n",
            "163 [D loss: 0.738421, acc.: 34.38%] [G loss: 0.780665]\n",
            "164 [D loss: 0.689717, acc.: 37.50%] [G loss: 0.941370]\n",
            "165 [D loss: 0.782785, acc.: 25.00%] [G loss: 0.899955]\n",
            "166 [D loss: 0.807005, acc.: 34.38%] [G loss: 0.804398]\n",
            "167 [D loss: 0.742814, acc.: 31.25%] [G loss: 0.791243]\n",
            "168 [D loss: 0.700888, acc.: 40.62%] [G loss: 0.790959]\n",
            "169 [D loss: 0.743379, acc.: 21.88%] [G loss: 0.873867]\n",
            "170 [D loss: 0.722961, acc.: 34.38%] [G loss: 0.773689]\n",
            "171 [D loss: 0.737004, acc.: 37.50%] [G loss: 0.850373]\n",
            "172 [D loss: 0.772349, acc.: 31.25%] [G loss: 0.868941]\n",
            "173 [D loss: 0.725638, acc.: 37.50%] [G loss: 0.861001]\n",
            "174 [D loss: 0.745444, acc.: 31.25%] [G loss: 0.800230]\n",
            "175 [D loss: 0.816961, acc.: 25.00%] [G loss: 0.749298]\n",
            "176 [D loss: 0.675186, acc.: 28.12%] [G loss: 0.807859]\n",
            "177 [D loss: 0.676927, acc.: 34.38%] [G loss: 0.887770]\n",
            "178 [D loss: 0.732430, acc.: 31.25%] [G loss: 0.797329]\n",
            "179 [D loss: 0.771284, acc.: 40.62%] [G loss: 0.817476]\n",
            "180 [D loss: 0.758344, acc.: 21.88%] [G loss: 0.796044]\n",
            "181 [D loss: 0.706695, acc.: 31.25%] [G loss: 0.806165]\n",
            "182 [D loss: 0.747910, acc.: 21.88%] [G loss: 0.853914]\n",
            "183 [D loss: 0.682993, acc.: 40.62%] [G loss: 0.702202]\n",
            "184 [D loss: 0.736788, acc.: 31.25%] [G loss: 0.746565]\n",
            "185 [D loss: 0.776917, acc.: 21.88%] [G loss: 0.832296]\n",
            "186 [D loss: 0.723473, acc.: 31.25%] [G loss: 0.694784]\n",
            "187 [D loss: 0.775916, acc.: 25.00%] [G loss: 0.714466]\n",
            "188 [D loss: 0.795125, acc.: 18.75%] [G loss: 0.732092]\n",
            "189 [D loss: 0.740209, acc.: 34.38%] [G loss: 0.833547]\n",
            "190 [D loss: 0.737723, acc.: 28.12%] [G loss: 0.773998]\n",
            "191 [D loss: 0.741330, acc.: 28.12%] [G loss: 0.761653]\n",
            "192 [D loss: 0.738533, acc.: 31.25%] [G loss: 0.885127]\n",
            "193 [D loss: 0.690148, acc.: 37.50%] [G loss: 0.796796]\n",
            "194 [D loss: 0.694780, acc.: 40.62%] [G loss: 0.847693]\n",
            "195 [D loss: 0.721755, acc.: 34.38%] [G loss: 0.881326]\n",
            "196 [D loss: 0.704888, acc.: 28.12%] [G loss: 0.794538]\n",
            "197 [D loss: 0.732239, acc.: 34.38%] [G loss: 0.867104]\n",
            "198 [D loss: 0.673395, acc.: 37.50%] [G loss: 0.655138]\n",
            "199 [D loss: 0.765102, acc.: 34.38%] [G loss: 0.757060]\n",
            "200 [D loss: 0.746921, acc.: 28.12%] [G loss: 0.770102]\n",
            "201 [D loss: 0.722615, acc.: 31.25%] [G loss: 0.823721]\n",
            "202 [D loss: 0.742071, acc.: 34.38%] [G loss: 0.796145]\n",
            "203 [D loss: 0.712034, acc.: 37.50%] [G loss: 0.727105]\n",
            "204 [D loss: 0.677638, acc.: 28.12%] [G loss: 0.740339]\n",
            "205 [D loss: 0.722218, acc.: 25.00%] [G loss: 0.701274]\n",
            "206 [D loss: 0.766076, acc.: 25.00%] [G loss: 0.792416]\n",
            "207 [D loss: 0.701890, acc.: 34.38%] [G loss: 0.762888]\n",
            "208 [D loss: 0.715748, acc.: 25.00%] [G loss: 0.798699]\n",
            "209 [D loss: 0.670694, acc.: 37.50%] [G loss: 0.804025]\n",
            "210 [D loss: 0.688353, acc.: 40.62%] [G loss: 0.870245]\n",
            "211 [D loss: 0.682493, acc.: 37.50%] [G loss: 0.804359]\n",
            "212 [D loss: 0.757297, acc.: 31.25%] [G loss: 0.870935]\n",
            "213 [D loss: 0.716134, acc.: 31.25%] [G loss: 0.859567]\n",
            "214 [D loss: 0.729000, acc.: 37.50%] [G loss: 0.776331]\n",
            "215 [D loss: 0.683444, acc.: 37.50%] [G loss: 0.859770]\n",
            "216 [D loss: 0.732384, acc.: 31.25%] [G loss: 0.738121]\n",
            "217 [D loss: 0.721760, acc.: 21.88%] [G loss: 0.866115]\n",
            "218 [D loss: 0.714249, acc.: 25.00%] [G loss: 0.845744]\n",
            "219 [D loss: 0.644217, acc.: 50.00%] [G loss: 0.917199]\n",
            "220 [D loss: 0.811788, acc.: 15.62%] [G loss: 0.726503]\n",
            "221 [D loss: 0.723151, acc.: 31.25%] [G loss: 0.816153]\n",
            "222 [D loss: 0.769917, acc.: 25.00%] [G loss: 0.856921]\n",
            "223 [D loss: 0.651863, acc.: 37.50%] [G loss: 0.889398]\n",
            "224 [D loss: 0.723438, acc.: 31.25%] [G loss: 0.789953]\n",
            "225 [D loss: 0.795282, acc.: 21.88%] [G loss: 0.790665]\n",
            "226 [D loss: 0.690293, acc.: 34.38%] [G loss: 0.796824]\n",
            "227 [D loss: 0.716836, acc.: 28.12%] [G loss: 0.833028]\n",
            "228 [D loss: 0.725364, acc.: 34.38%] [G loss: 0.747798]\n",
            "229 [D loss: 0.687097, acc.: 40.62%] [G loss: 0.817097]\n",
            "230 [D loss: 0.773733, acc.: 31.25%] [G loss: 0.747635]\n",
            "231 [D loss: 0.690041, acc.: 40.62%] [G loss: 0.806590]\n",
            "232 [D loss: 0.753992, acc.: 25.00%] [G loss: 0.836553]\n",
            "233 [D loss: 0.703420, acc.: 25.00%] [G loss: 0.815196]\n",
            "234 [D loss: 0.716196, acc.: 31.25%] [G loss: 0.804188]\n",
            "235 [D loss: 0.811171, acc.: 18.75%] [G loss: 0.729864]\n",
            "236 [D loss: 0.660121, acc.: 34.38%] [G loss: 0.891925]\n",
            "237 [D loss: 0.756623, acc.: 34.38%] [G loss: 0.816593]\n",
            "238 [D loss: 0.760914, acc.: 34.38%] [G loss: 0.783736]\n",
            "239 [D loss: 0.728280, acc.: 28.12%] [G loss: 0.888829]\n",
            "240 [D loss: 0.733651, acc.: 21.88%] [G loss: 0.784061]\n",
            "241 [D loss: 0.742791, acc.: 37.50%] [G loss: 0.800552]\n",
            "242 [D loss: 0.710517, acc.: 37.50%] [G loss: 0.758966]\n",
            "243 [D loss: 0.711819, acc.: 28.12%] [G loss: 0.746833]\n",
            "244 [D loss: 0.693889, acc.: 34.38%] [G loss: 0.804455]\n",
            "245 [D loss: 0.754122, acc.: 37.50%] [G loss: 0.717386]\n",
            "246 [D loss: 0.714667, acc.: 28.12%] [G loss: 0.801122]\n",
            "247 [D loss: 0.706452, acc.: 31.25%] [G loss: 0.802337]\n",
            "248 [D loss: 0.706475, acc.: 31.25%] [G loss: 0.810562]\n",
            "249 [D loss: 0.700643, acc.: 37.50%] [G loss: 0.893449]\n",
            "250 [D loss: 0.752320, acc.: 28.12%] [G loss: 0.813165]\n",
            "251 [D loss: 0.693408, acc.: 37.50%] [G loss: 0.758293]\n",
            "252 [D loss: 0.649688, acc.: 37.50%] [G loss: 0.832116]\n",
            "253 [D loss: 0.706166, acc.: 34.38%] [G loss: 0.799478]\n",
            "254 [D loss: 0.679701, acc.: 43.75%] [G loss: 0.798767]\n",
            "255 [D loss: 0.677015, acc.: 43.75%] [G loss: 0.780475]\n",
            "256 [D loss: 0.808642, acc.: 37.50%] [G loss: 0.819480]\n",
            "257 [D loss: 0.785873, acc.: 34.38%] [G loss: 0.808818]\n",
            "258 [D loss: 0.702409, acc.: 37.50%] [G loss: 0.734346]\n",
            "259 [D loss: 0.720968, acc.: 28.12%] [G loss: 0.844865]\n",
            "260 [D loss: 0.730834, acc.: 34.38%] [G loss: 0.794377]\n",
            "261 [D loss: 0.703281, acc.: 31.25%] [G loss: 0.726902]\n",
            "262 [D loss: 0.631533, acc.: 37.50%] [G loss: 0.829290]\n",
            "263 [D loss: 0.726717, acc.: 28.12%] [G loss: 0.754226]\n",
            "264 [D loss: 0.746178, acc.: 25.00%] [G loss: 0.824438]\n",
            "265 [D loss: 0.612644, acc.: 40.62%] [G loss: 0.863785]\n",
            "266 [D loss: 0.806996, acc.: 18.75%] [G loss: 0.785783]\n",
            "267 [D loss: 0.676100, acc.: 40.62%] [G loss: 0.817638]\n",
            "268 [D loss: 0.789272, acc.: 40.62%] [G loss: 0.736831]\n",
            "269 [D loss: 0.681753, acc.: 40.62%] [G loss: 0.756966]\n",
            "270 [D loss: 0.744483, acc.: 31.25%] [G loss: 0.813505]\n",
            "271 [D loss: 0.690309, acc.: 28.12%] [G loss: 0.883390]\n",
            "272 [D loss: 0.705444, acc.: 40.62%] [G loss: 0.758246]\n",
            "273 [D loss: 0.696864, acc.: 28.12%] [G loss: 0.814966]\n",
            "274 [D loss: 0.702878, acc.: 34.38%] [G loss: 0.775733]\n",
            "275 [D loss: 0.637258, acc.: 40.62%] [G loss: 0.825554]\n",
            "276 [D loss: 0.681933, acc.: 40.62%] [G loss: 0.781527]\n",
            "277 [D loss: 0.755916, acc.: 25.00%] [G loss: 0.754726]\n",
            "278 [D loss: 0.717818, acc.: 40.62%] [G loss: 0.838998]\n",
            "279 [D loss: 0.803679, acc.: 18.75%] [G loss: 0.836944]\n",
            "280 [D loss: 0.738853, acc.: 37.50%] [G loss: 0.828639]\n",
            "281 [D loss: 0.737939, acc.: 40.62%] [G loss: 0.846030]\n",
            "282 [D loss: 0.678360, acc.: 40.62%] [G loss: 0.813347]\n",
            "283 [D loss: 0.695238, acc.: 34.38%] [G loss: 0.857746]\n",
            "284 [D loss: 0.683051, acc.: 34.38%] [G loss: 0.784196]\n",
            "285 [D loss: 0.700221, acc.: 43.75%] [G loss: 0.768014]\n",
            "286 [D loss: 0.672675, acc.: 34.38%] [G loss: 0.883638]\n",
            "287 [D loss: 0.739215, acc.: 31.25%] [G loss: 0.804127]\n",
            "288 [D loss: 0.662474, acc.: 40.62%] [G loss: 0.838649]\n",
            "289 [D loss: 0.762942, acc.: 25.00%] [G loss: 0.889949]\n",
            "290 [D loss: 0.648024, acc.: 37.50%] [G loss: 0.820454]\n",
            "291 [D loss: 0.672644, acc.: 40.62%] [G loss: 0.804815]\n",
            "292 [D loss: 0.695257, acc.: 28.12%] [G loss: 0.744633]\n",
            "293 [D loss: 0.785815, acc.: 37.50%] [G loss: 0.852079]\n",
            "294 [D loss: 0.784218, acc.: 21.88%] [G loss: 0.738242]\n",
            "295 [D loss: 0.747982, acc.: 25.00%] [G loss: 0.787287]\n",
            "296 [D loss: 0.668302, acc.: 40.62%] [G loss: 0.838274]\n",
            "297 [D loss: 0.689282, acc.: 34.38%] [G loss: 0.909293]\n",
            "298 [D loss: 0.702914, acc.: 28.12%] [G loss: 0.859067]\n",
            "299 [D loss: 0.750115, acc.: 28.12%] [G loss: 0.777425]\n",
            "300 [D loss: 0.666783, acc.: 37.50%] [G loss: 0.663650]\n",
            "301 [D loss: 0.693775, acc.: 40.62%] [G loss: 0.844177]\n",
            "302 [D loss: 0.789531, acc.: 28.12%] [G loss: 0.816176]\n",
            "303 [D loss: 0.719067, acc.: 37.50%] [G loss: 0.879117]\n",
            "304 [D loss: 0.721407, acc.: 37.50%] [G loss: 0.791023]\n",
            "305 [D loss: 0.639845, acc.: 43.75%] [G loss: 0.813723]\n",
            "306 [D loss: 0.744908, acc.: 21.88%] [G loss: 0.780444]\n",
            "307 [D loss: 0.745924, acc.: 28.12%] [G loss: 0.765289]\n",
            "308 [D loss: 0.736241, acc.: 34.38%] [G loss: 0.828079]\n",
            "309 [D loss: 0.696944, acc.: 43.75%] [G loss: 0.790279]\n",
            "310 [D loss: 0.714906, acc.: 31.25%] [G loss: 0.854375]\n",
            "311 [D loss: 0.740626, acc.: 34.38%] [G loss: 0.766880]\n",
            "312 [D loss: 0.714749, acc.: 31.25%] [G loss: 0.878288]\n",
            "313 [D loss: 0.695948, acc.: 34.38%] [G loss: 0.767616]\n",
            "314 [D loss: 0.739978, acc.: 34.38%] [G loss: 0.843679]\n",
            "315 [D loss: 0.700614, acc.: 31.25%] [G loss: 0.848139]\n",
            "316 [D loss: 0.719054, acc.: 31.25%] [G loss: 0.899568]\n",
            "317 [D loss: 0.749439, acc.: 34.38%] [G loss: 0.857658]\n",
            "318 [D loss: 0.759520, acc.: 21.88%] [G loss: 0.839471]\n",
            "319 [D loss: 0.700456, acc.: 34.38%] [G loss: 0.738455]\n",
            "320 [D loss: 0.708032, acc.: 34.38%] [G loss: 0.827473]\n",
            "321 [D loss: 0.697132, acc.: 40.62%] [G loss: 0.827576]\n",
            "322 [D loss: 0.695533, acc.: 43.75%] [G loss: 0.828779]\n",
            "323 [D loss: 0.748366, acc.: 31.25%] [G loss: 0.763215]\n",
            "324 [D loss: 0.733397, acc.: 28.12%] [G loss: 0.794630]\n",
            "325 [D loss: 0.728322, acc.: 25.00%] [G loss: 0.754233]\n",
            "326 [D loss: 0.713023, acc.: 34.38%] [G loss: 0.817920]\n",
            "327 [D loss: 0.743300, acc.: 31.25%] [G loss: 0.806649]\n",
            "328 [D loss: 0.755450, acc.: 21.88%] [G loss: 0.815156]\n",
            "329 [D loss: 0.708156, acc.: 34.38%] [G loss: 0.727069]\n",
            "330 [D loss: 0.708436, acc.: 31.25%] [G loss: 0.716000]\n",
            "331 [D loss: 0.722528, acc.: 31.25%] [G loss: 0.817802]\n",
            "332 [D loss: 0.706897, acc.: 28.12%] [G loss: 0.795434]\n",
            "333 [D loss: 0.650428, acc.: 43.75%] [G loss: 0.813227]\n",
            "334 [D loss: 0.679947, acc.: 31.25%] [G loss: 0.857422]\n",
            "335 [D loss: 0.697164, acc.: 37.50%] [G loss: 0.740061]\n",
            "336 [D loss: 0.724349, acc.: 25.00%] [G loss: 0.744116]\n",
            "337 [D loss: 0.670043, acc.: 40.62%] [G loss: 0.873352]\n",
            "338 [D loss: 0.688420, acc.: 37.50%] [G loss: 0.790452]\n",
            "339 [D loss: 0.738444, acc.: 37.50%] [G loss: 0.883749]\n",
            "340 [D loss: 0.756913, acc.: 31.25%] [G loss: 0.781341]\n",
            "341 [D loss: 0.739556, acc.: 31.25%] [G loss: 0.752754]\n",
            "342 [D loss: 0.701111, acc.: 28.12%] [G loss: 0.789216]\n",
            "343 [D loss: 0.655332, acc.: 37.50%] [G loss: 0.802665]\n",
            "344 [D loss: 0.768294, acc.: 31.25%] [G loss: 0.734895]\n",
            "345 [D loss: 0.680045, acc.: 37.50%] [G loss: 0.773622]\n",
            "346 [D loss: 0.713726, acc.: 40.62%] [G loss: 0.796255]\n",
            "347 [D loss: 0.745387, acc.: 28.12%] [G loss: 0.830109]\n",
            "348 [D loss: 0.829635, acc.: 21.88%] [G loss: 0.807345]\n",
            "349 [D loss: 0.720107, acc.: 37.50%] [G loss: 0.762001]\n",
            "350 [D loss: 0.658668, acc.: 46.88%] [G loss: 0.776189]\n",
            "351 [D loss: 0.706264, acc.: 28.12%] [G loss: 0.771178]\n",
            "352 [D loss: 0.739492, acc.: 28.12%] [G loss: 0.794527]\n",
            "353 [D loss: 0.761889, acc.: 31.25%] [G loss: 0.725422]\n",
            "354 [D loss: 0.698482, acc.: 43.75%] [G loss: 0.814887]\n",
            "355 [D loss: 0.721746, acc.: 28.12%] [G loss: 0.803436]\n",
            "356 [D loss: 0.644375, acc.: 43.75%] [G loss: 0.808205]\n",
            "357 [D loss: 0.725597, acc.: 34.38%] [G loss: 0.866818]\n",
            "358 [D loss: 0.715464, acc.: 37.50%] [G loss: 0.794494]\n",
            "359 [D loss: 0.787816, acc.: 34.38%] [G loss: 0.824655]\n",
            "360 [D loss: 0.745858, acc.: 34.38%] [G loss: 0.900312]\n",
            "361 [D loss: 0.725443, acc.: 37.50%] [G loss: 0.814422]\n",
            "362 [D loss: 0.687813, acc.: 31.25%] [G loss: 0.820022]\n",
            "363 [D loss: 0.728348, acc.: 34.38%] [G loss: 0.791839]\n",
            "364 [D loss: 0.730933, acc.: 34.38%] [G loss: 0.786045]\n",
            "365 [D loss: 0.713148, acc.: 31.25%] [G loss: 0.822249]\n",
            "366 [D loss: 0.698680, acc.: 25.00%] [G loss: 0.759711]\n",
            "367 [D loss: 0.719575, acc.: 40.62%] [G loss: 0.842409]\n",
            "368 [D loss: 0.706364, acc.: 28.12%] [G loss: 0.761753]\n",
            "369 [D loss: 0.693762, acc.: 43.75%] [G loss: 0.788246]\n",
            "370 [D loss: 0.678985, acc.: 37.50%] [G loss: 0.809441]\n",
            "371 [D loss: 0.720743, acc.: 34.38%] [G loss: 0.757194]\n",
            "372 [D loss: 0.732578, acc.: 28.12%] [G loss: 0.832105]\n",
            "373 [D loss: 0.693197, acc.: 31.25%] [G loss: 0.794688]\n",
            "374 [D loss: 0.717832, acc.: 31.25%] [G loss: 0.809468]\n",
            "375 [D loss: 0.705648, acc.: 25.00%] [G loss: 0.849655]\n",
            "376 [D loss: 0.714548, acc.: 34.38%] [G loss: 0.766236]\n",
            "377 [D loss: 0.709141, acc.: 37.50%] [G loss: 0.836274]\n",
            "378 [D loss: 0.697526, acc.: 37.50%] [G loss: 0.886157]\n",
            "379 [D loss: 0.701905, acc.: 37.50%] [G loss: 0.820395]\n",
            "380 [D loss: 0.679223, acc.: 31.25%] [G loss: 0.791369]\n",
            "381 [D loss: 0.734038, acc.: 43.75%] [G loss: 0.808913]\n",
            "382 [D loss: 0.764328, acc.: 37.50%] [G loss: 0.917415]\n",
            "383 [D loss: 0.724441, acc.: 28.12%] [G loss: 0.772353]\n",
            "384 [D loss: 0.755293, acc.: 21.88%] [G loss: 0.823190]\n",
            "385 [D loss: 0.729559, acc.: 28.12%] [G loss: 0.749755]\n",
            "386 [D loss: 0.678230, acc.: 31.25%] [G loss: 0.865668]\n",
            "387 [D loss: 0.756541, acc.: 25.00%] [G loss: 0.692466]\n",
            "388 [D loss: 0.801479, acc.: 28.12%] [G loss: 0.841260]\n",
            "389 [D loss: 0.647494, acc.: 40.62%] [G loss: 0.825320]\n",
            "390 [D loss: 0.698560, acc.: 31.25%] [G loss: 0.815806]\n",
            "391 [D loss: 0.711876, acc.: 31.25%] [G loss: 0.739232]\n",
            "392 [D loss: 0.650602, acc.: 34.38%] [G loss: 0.838866]\n",
            "393 [D loss: 0.724862, acc.: 40.62%] [G loss: 0.820055]\n",
            "394 [D loss: 0.785357, acc.: 31.25%] [G loss: 0.752888]\n",
            "395 [D loss: 0.766146, acc.: 25.00%] [G loss: 0.785564]\n",
            "396 [D loss: 0.682127, acc.: 40.62%] [G loss: 0.785488]\n",
            "397 [D loss: 0.730489, acc.: 40.62%] [G loss: 0.863439]\n",
            "398 [D loss: 0.739665, acc.: 31.25%] [G loss: 0.856492]\n",
            "399 [D loss: 0.705090, acc.: 31.25%] [G loss: 0.839296]\n",
            "400 [D loss: 0.641956, acc.: 34.38%] [G loss: 0.778040]\n",
            "401 [D loss: 0.680005, acc.: 34.38%] [G loss: 0.784454]\n",
            "402 [D loss: 0.671799, acc.: 34.38%] [G loss: 0.789347]\n",
            "403 [D loss: 0.759260, acc.: 31.25%] [G loss: 0.844379]\n",
            "404 [D loss: 0.680623, acc.: 31.25%] [G loss: 0.742771]\n",
            "405 [D loss: 0.773615, acc.: 18.75%] [G loss: 0.853875]\n",
            "406 [D loss: 0.718855, acc.: 28.12%] [G loss: 0.820905]\n",
            "407 [D loss: 0.738220, acc.: 34.38%] [G loss: 0.844378]\n",
            "408 [D loss: 0.734528, acc.: 37.50%] [G loss: 0.851901]\n",
            "409 [D loss: 0.709561, acc.: 37.50%] [G loss: 0.790328]\n",
            "410 [D loss: 0.708888, acc.: 34.38%] [G loss: 0.839721]\n",
            "411 [D loss: 0.599384, acc.: 43.75%] [G loss: 0.795429]\n",
            "412 [D loss: 0.721312, acc.: 37.50%] [G loss: 0.803369]\n",
            "413 [D loss: 0.689445, acc.: 34.38%] [G loss: 0.882386]\n",
            "414 [D loss: 0.690075, acc.: 34.38%] [G loss: 0.803041]\n",
            "415 [D loss: 0.674495, acc.: 34.38%] [G loss: 0.766780]\n",
            "416 [D loss: 0.749142, acc.: 21.88%] [G loss: 0.742248]\n",
            "417 [D loss: 0.708962, acc.: 25.00%] [G loss: 0.899468]\n",
            "418 [D loss: 0.710772, acc.: 28.12%] [G loss: 0.762235]\n",
            "419 [D loss: 0.720259, acc.: 31.25%] [G loss: 0.780038]\n",
            "420 [D loss: 0.766331, acc.: 28.12%] [G loss: 0.795428]\n",
            "421 [D loss: 0.713615, acc.: 28.12%] [G loss: 0.808725]\n",
            "422 [D loss: 0.770613, acc.: 31.25%] [G loss: 0.784905]\n",
            "423 [D loss: 0.723233, acc.: 40.62%] [G loss: 0.879017]\n",
            "424 [D loss: 0.704280, acc.: 43.75%] [G loss: 0.833386]\n",
            "425 [D loss: 0.710347, acc.: 43.75%] [G loss: 0.843604]\n",
            "426 [D loss: 0.691375, acc.: 37.50%] [G loss: 0.788450]\n",
            "427 [D loss: 0.719049, acc.: 37.50%] [G loss: 0.848864]\n",
            "428 [D loss: 0.728215, acc.: 31.25%] [G loss: 0.797294]\n",
            "429 [D loss: 0.768686, acc.: 28.12%] [G loss: 0.808747]\n",
            "430 [D loss: 0.713632, acc.: 31.25%] [G loss: 0.776858]\n",
            "431 [D loss: 0.717710, acc.: 28.12%] [G loss: 0.873904]\n",
            "432 [D loss: 0.685450, acc.: 43.75%] [G loss: 0.830945]\n",
            "433 [D loss: 0.721881, acc.: 34.38%] [G loss: 0.753428]\n",
            "434 [D loss: 0.748079, acc.: 34.38%] [G loss: 0.859457]\n",
            "435 [D loss: 0.661587, acc.: 43.75%] [G loss: 0.754579]\n",
            "436 [D loss: 0.759504, acc.: 31.25%] [G loss: 0.856268]\n",
            "437 [D loss: 0.701348, acc.: 40.62%] [G loss: 0.758811]\n",
            "438 [D loss: 0.689300, acc.: 34.38%] [G loss: 0.758099]\n",
            "439 [D loss: 0.763325, acc.: 28.12%] [G loss: 0.714368]\n",
            "440 [D loss: 0.710271, acc.: 34.38%] [G loss: 0.772343]\n",
            "441 [D loss: 0.735580, acc.: 25.00%] [G loss: 0.767624]\n",
            "442 [D loss: 0.765632, acc.: 34.38%] [G loss: 0.825823]\n",
            "443 [D loss: 0.679218, acc.: 31.25%] [G loss: 0.791046]\n",
            "444 [D loss: 0.692063, acc.: 28.12%] [G loss: 0.769622]\n",
            "445 [D loss: 0.703784, acc.: 34.38%] [G loss: 0.754533]\n",
            "446 [D loss: 0.706882, acc.: 34.38%] [G loss: 0.794814]\n",
            "447 [D loss: 0.748891, acc.: 34.38%] [G loss: 0.760922]\n",
            "448 [D loss: 0.712462, acc.: 31.25%] [G loss: 0.800516]\n",
            "449 [D loss: 0.701092, acc.: 34.38%] [G loss: 0.845124]\n",
            "450 [D loss: 0.758827, acc.: 25.00%] [G loss: 0.869964]\n",
            "451 [D loss: 0.705173, acc.: 31.25%] [G loss: 0.775706]\n",
            "452 [D loss: 0.727035, acc.: 28.12%] [G loss: 0.871712]\n",
            "453 [D loss: 0.673132, acc.: 40.62%] [G loss: 0.770822]\n",
            "454 [D loss: 0.716286, acc.: 31.25%] [G loss: 0.813510]\n",
            "455 [D loss: 0.727296, acc.: 28.12%] [G loss: 0.846720]\n",
            "456 [D loss: 0.687362, acc.: 40.62%] [G loss: 0.895408]\n",
            "457 [D loss: 0.685841, acc.: 40.62%] [G loss: 0.856338]\n",
            "458 [D loss: 0.769241, acc.: 21.88%] [G loss: 0.784528]\n",
            "459 [D loss: 0.751813, acc.: 34.38%] [G loss: 0.780823]\n",
            "460 [D loss: 0.753103, acc.: 28.12%] [G loss: 0.810749]\n",
            "461 [D loss: 0.721630, acc.: 21.88%] [G loss: 0.798521]\n",
            "462 [D loss: 0.746835, acc.: 28.12%] [G loss: 0.784877]\n",
            "463 [D loss: 0.719132, acc.: 37.50%] [G loss: 0.853221]\n",
            "464 [D loss: 0.697033, acc.: 37.50%] [G loss: 0.761467]\n",
            "465 [D loss: 0.598046, acc.: 43.75%] [G loss: 0.856347]\n",
            "466 [D loss: 0.677425, acc.: 37.50%] [G loss: 0.820699]\n",
            "467 [D loss: 0.705534, acc.: 37.50%] [G loss: 0.802750]\n",
            "468 [D loss: 0.777676, acc.: 25.00%] [G loss: 0.832971]\n",
            "469 [D loss: 0.702907, acc.: 25.00%] [G loss: 0.841379]\n",
            "470 [D loss: 0.682762, acc.: 43.75%] [G loss: 0.791084]\n",
            "471 [D loss: 0.678699, acc.: 43.75%] [G loss: 0.859815]\n",
            "472 [D loss: 0.705264, acc.: 37.50%] [G loss: 0.779067]\n",
            "473 [D loss: 0.692207, acc.: 34.38%] [G loss: 0.788010]\n",
            "474 [D loss: 0.692984, acc.: 28.12%] [G loss: 0.694669]\n",
            "475 [D loss: 0.723449, acc.: 31.25%] [G loss: 0.796212]\n",
            "476 [D loss: 0.737964, acc.: 34.38%] [G loss: 0.756267]\n",
            "477 [D loss: 0.674169, acc.: 25.00%] [G loss: 0.843236]\n",
            "478 [D loss: 0.664997, acc.: 31.25%] [G loss: 0.753081]\n",
            "479 [D loss: 0.686730, acc.: 34.38%] [G loss: 0.823194]\n",
            "480 [D loss: 0.698793, acc.: 31.25%] [G loss: 0.879283]\n",
            "481 [D loss: 0.665887, acc.: 37.50%] [G loss: 0.736113]\n",
            "482 [D loss: 0.692540, acc.: 37.50%] [G loss: 0.907642]\n",
            "483 [D loss: 0.741312, acc.: 31.25%] [G loss: 0.846546]\n",
            "484 [D loss: 0.698204, acc.: 40.62%] [G loss: 0.812547]\n",
            "485 [D loss: 0.630499, acc.: 43.75%] [G loss: 0.876032]\n",
            "486 [D loss: 0.749141, acc.: 31.25%] [G loss: 0.838623]\n",
            "487 [D loss: 0.771078, acc.: 25.00%] [G loss: 0.783756]\n",
            "488 [D loss: 0.727436, acc.: 37.50%] [G loss: 0.899559]\n",
            "489 [D loss: 0.739339, acc.: 28.12%] [G loss: 0.783613]\n",
            "490 [D loss: 0.723092, acc.: 34.38%] [G loss: 0.847773]\n",
            "491 [D loss: 0.673624, acc.: 34.38%] [G loss: 0.790020]\n",
            "492 [D loss: 0.686798, acc.: 40.62%] [G loss: 0.817128]\n",
            "493 [D loss: 0.769891, acc.: 31.25%] [G loss: 0.807259]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXIkuMGUv88g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}